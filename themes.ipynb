{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd66bb0-2679-4665-a1a7-af36d9cfeb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import llmcode\n",
    "from IPython.display import HTML, Markdown, display\n",
    "import getpass\n",
    "import os\n",
    "import html\n",
    "import lxml\n",
    "import re\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import textwrap\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "from itertools import chain\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e978d56-1130-405d-8992-2db84230a8e5",
   "metadata": {},
   "source": [
    "Init the LLMCode library. Set the llm_API variable to \"Aalto\" to use Aalto's GDPR-safe Azure OpenAI API endpoints that are suitable for processing confidential data. Here, we use the OpenAI API because it is faster and makes this notebook usable for people outside Aalto.\n",
    "\n",
    "When you run the code, it will ask you to input an appropriate API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cc3c67-3d91-4f89-95db-62a5abdabcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_API=\"OpenAI\"\n",
    "if llm_API==\"OpenAI\":\n",
    "    if os.environ.get(\"OPENAI_API_KEY\") is None:\n",
    "        print(\"Please input an OpenAI API key\")\n",
    "        api_key = getpass.getpass()\n",
    "        os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "elif llm_API==\"Aalto\":\n",
    "    if os.environ.get(\"AALTO_OPENAI_API_KEY\") is None:\n",
    "        print(\"Please input an Aalto OpenAI API key\")\n",
    "        api_key = getpass.getpass()\n",
    "        os.environ[\"AALTO_OPENAI_API_KEY\"] = api_key\n",
    "else:\n",
    "    print(f\"Invalid API type: {llm_API}\")\n",
    "llmcode.init(API=llm_API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bfd5be-f93d-4888-a9c2-e38383346171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter is already running an asyncio event loop => need this hack for async OpenAI API calling\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabecd76-cee0-4473-a9fd-4826d6fb533b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the GPT model to use\n",
    "gpt_model = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3331f4e2-b5d2-4f09-b427-6f5503339853",
   "metadata": {},
   "source": [
    "# Load codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b713500f-12c4-40c1-bbcb-1d0276131720",
   "metadata": {},
   "source": [
    "Let's start with defining the research question and loading the codes you created in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401d6a46-a3fc-4b70-8c0e-beb1ea49f070",
   "metadata": {},
   "outputs": [],
   "source": [
    "research_question = \"How do people experience games as art?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192b0969-3350-4a19-8f67-1d476cd7b4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "coded_texts_path = \"coding_output/coded_texts_inductive_with_code_consistency.csv\"\n",
    "code_descriptions_path = \"coding_output/code_descriptions_inductive_with_code_consistency.csv\"\n",
    "\n",
    "coded_texts_df = pd.read_csv(coded_texts_path)\n",
    "coded_texts = coded_texts_df.coded_text.tolist()\n",
    "\n",
    "code_descriptions_df = pd.read_csv(code_descriptions_path)\n",
    "code_descriptions = dict(zip(code_descriptions_df.code, code_descriptions_df.description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cebe5d-1dcd-4bf2-a432-3dc33c67e998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse all codes and highlights in LLM output\n",
    "code_highlights = llmcode.get_codes_and_highlights(coded_texts)\n",
    "\n",
    "# Print all LLM-created codes\n",
    "print(\"\\nLLM-generated codes:\\n\")\n",
    "for code, highlights in sorted(code_highlights.items()):\n",
    "    print(f\"{code} ({len(highlights)}): {code_descriptions[code]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc39853-e234-4e38-8246-d407ee49a7fb",
   "metadata": {},
   "source": [
    "# Generate themes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848c547b-33a5-48b5-bdbb-48e9b445d92e",
   "metadata": {},
   "source": [
    "In this notebook, we use LLMs to group the codes you generated in the previous notebook under wider themes, as is often done as part of qualitative analysis.\n",
    "\n",
    "Before we begin, let's take a moment to critically reflect on the role of agency and transparency when using LLMs for this purpose. While LLMs can efficiently organize large amounts of data, they may lack the nuanced understanding of human context and intentions. This raises concerns about researcher agencyâ€”how much control do researchers retain over the interpretation of their data? Similarly, the transparency of LLMs' decision-making processes is limited, making it difficult to trace how specific themes were generated, which may obscure valuable insights or introduce unintended biases. Balancing automation with researcher input is crucial to maintain both rigor and interpretive depth in the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9464efd2-ff67-4ede-b327-2e4a8f5bd754",
   "metadata": {},
   "source": [
    "## Simple example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652ab5f8-50b9-4678-a535-3535b26c61d8",
   "metadata": {},
   "source": [
    "Let's first look at a simple example of how an LLM may be prompted to generate themes for a set of codes. Feel free to modify the prompt to see how the output changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432791a8-9a99-407f-bf14-0fc4df42c90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are an expert qualitative researcher. You are given a list of qualitative codes at the end of this prompt. Please carry out the following task:\n",
    "- Group these codes into overearching themes that relate to the research question.\n",
    "- Assign codes to the themes provided in the list of user-defined themes and generate new themes when needed.\n",
    "- The theme names should be detailed and expressive.\n",
    "- Output a list of Theme objects, containing the theme name and a list of codes that are included in that theme. Start this list with the user-defined themes.\n",
    "- Include each of the codes under exactly one theme.\n",
    "- Give your output as valid JSON.\n",
    "\n",
    "THEME EXAMPLES:\n",
    "Appreciation of Craftsmanship and Aesthetics\n",
    "Interactive Experience and Player Involvement\n",
    "\n",
    "CODES:\n",
    "novelty\n",
    "player agency\n",
    "realism\n",
    "craftsmanship\n",
    "sacrifice\n",
    "setting\n",
    "beauty\n",
    "\n",
    "RESEARCH QUESTION: How do people experience games as art?\n",
    "\"\"\"\n",
    "\n",
    "response=llmcode.query_LLM(prompt, model=gpt_model)\n",
    "print(\"LLM output:\\n\")\n",
    "try:\n",
    "    display(Markdown(response))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b59102-38f4-456c-8c83-871bae4153c4",
   "metadata": {},
   "source": [
    "## Generating themes with LLMCode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0bb569-000b-409d-9083-4eb1b13f4abd",
   "metadata": {},
   "source": [
    "We want the LLM to output data in a structured format, i.e. a list of themes each containing a theme name and a sub-list of codes. The output from LLMs is not always perfect, which means that it is best to use LLMCode's get_themes() function for this task that automatically corrects such cases.\n",
    "\n",
    "For example, sometimes, the LLM the may be unable to assign all codes under a theme in one pass. Particularly with long inputs, in this case a potentially lengthy list of codes, the attention mechanism underlying LLMs may not be able to \"focus\" on all of the codes at once. One solution is to solve the task iteratively: we can set max_retries to an integer N to make the function repeat the analysis up to N times for the unassigned codes.\n",
    "\n",
    "Alternatively, you may wish to take part in assigning the unthemed codes yourself, in what is referred to as the [human-in-the-loop](https://en.wikipedia.org/wiki/Human-in-the-loop) approach. To do so, you may set max_retries=0 and assign the codes in unthemed_codes to the themes yourself.\n",
    "\n",
    "The following list gives an overview of some of the customisable parameters to get_themes():\n",
    "\n",
    "* codes (list): A list of codes to be thematically grouped.\n",
    "* prior_themes (dict or list): Existing themes to which the LLM should add new codes. If a dict is provided, keys are theme names and values are sets of codes already associated with each theme. If a list is provided, it's assumed to be a list of theme names with no associated codes.\n",
    "* code_descriptions (dict, optional): Optional descriptions for each code, providing additional context to the LLM.\n",
    "* max_retries (int, optional): Maximum number of retries to attempt assigning themes to all codes. Defaults to 0 (no retries).\n",
    "\n",
    "Here, using the advanced GPT-4o model over smaller models like GPT-4o-mini is recommended due to the complexity of the task. Larger models are able to utilise attention more effectively, and therefore handle a larger amount of input at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c465bae3-a966-4d18-b404-c92d12b44992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the themes, or leave the dictionary empty {} to have the LLM make the first pass\n",
    "themes = {\"How games fit within a traditional view of art\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fa704f-cf20-4a7d-b54a-be45176edf4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "codes = set(code_highlights.keys())\n",
    "\n",
    "themes, unthemed_codes = llmcode.get_themes(\n",
    "    codes=codes,\n",
    "    prior_themes=themes,\n",
    "    code_descriptions=code_descriptions,\n",
    "    max_retries=3,\n",
    "    research_question=research_question,\n",
    "    gpt_model=\"gpt-4o\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518a0e42-e35f-4af7-9fd3-03940ea805bd",
   "metadata": {},
   "source": [
    "Let's take a look at the generated themes, and check if any codes were left without a theme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe7da41-0237-495f-b613-fa79ea9c5921",
   "metadata": {},
   "outputs": [],
   "source": [
    "for theme, codes in themes.items():\n",
    "    print(f\"Theme: {theme}\")\n",
    "    print(\"Codes: \" + \"; \".join(codes))\n",
    "    print(\"\")\n",
    "\n",
    "if unthemed_codes:\n",
    "    print(f\"{len(unthemed_codes)} codes weren't assigned a theme: \" + \"; \".join(unthemed_codes))\n",
    "else:\n",
    "    print(\"All codes were assigned a theme.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9921453-19b9-4617-9666-c27c7debbef1",
   "metadata": {},
   "source": [
    "# Communicating the findings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cea4031-b151-4866-9e2b-cf820c6986c4",
   "metadata": {},
   "source": [
    "Finally, we take a look at two approaches to communicating the research findings. The first is simply producing a table of all the identified themes, and for each theme:\n",
    "* the list of included codes;\n",
    "* the number of mentions, calculated as the total count of all segments across the input texts annotated by any of the included codes; and\n",
    "* an example quotation chosen from the annotated segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3f2d16-82a6-428e-8736-63f716c8b46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce table\n",
    "theme_mentions = {theme: sum(len(code_highlights[code]) for code in themes[theme]) for theme in themes}\n",
    "theme_examples = {theme: random.choice(list(chain(*(code_highlights[code] for code in themes[theme])))) for theme in themes}\n",
    "theme_data = [(theme, \"; \" .join(codes), theme_mentions[theme], theme_examples[theme]) for theme, codes in themes.items()]\n",
    "pd.DataFrame(theme_data, columns=[\"theme\", \"codes\", \"mentions\", \"example quotation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf63d60-0e86-499c-8f91-324134b120e6",
   "metadata": {},
   "source": [
    "We can also ask LLMCode to write a report about the findings, using the themes, codes, and quotations as inputs. In academic writing, it is absolutely crucial that the LLM does not make up or \"hallucinate\" incorrect information. The write_report function automatically checks for and removes any hallucinated quotes from the output, but it is important for you as a researcher to verify that the findings here reflect your personal insights about the data. What other potential issues do you see with using LLMs to communicate research findings?\n",
    "\n",
    "The following list gives an overview of some of the customisable parameters to write_report():\n",
    "* themes (dict): A dictionary where keys are theme names and values are sets of codes that belong to each theme.\n",
    "* code_highlights (dict): A dictionary where keys are codes and values are lists of highlights (quotes or text) related to those codes.\n",
    "* max_themes (int, optional): The maximum number of themes to include in the report. If None, all themes with at least three highlights are included. Defaults to None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a89fb0-fffa-48ea-9f6e-3d53edf4a424",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "report = llmcode.write_report(\n",
    "    themes=themes,\n",
    "    code_highlights=code_highlights,\n",
    "    max_themes=4,\n",
    "    research_question=research_question,\n",
    "    gpt_model=gpt_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290c98f6-8ae3-4489-858e-06e853162d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(report))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
