{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial 2: Extracting or highlighting relevant text with LLMs\n",
        "\n",
        "*This notebook is part of the [LLMCode library](https://github.com/PerttuHamalainen/LLMCode).*\n",
        "\n",
        "*A note on data privacy: The user experience of this notebook is better on Google Colab, but if you are processing data that cannot be sent to Google and OpenAI servers, you should run this notebook locally using the \"Aalto\" LLM API.*\n",
        "\n",
        "**Learning goal:** A key step in qualitative data analysis is extracting or highlighting parts of texts (e.g., interviews, survey responses) that are relevant to one's research question. This typically precedes a subsequent analysis step such as assigning codes to the extracts.\n",
        "\n",
        "In this notebook, you'll learn how LLMs can be prompted to do the extraction based on your instructions and examples.\n",
        "\n",
        "**How to use this Colab notebook?**\n",
        "* If you are not familiar to Colab, please first practice with [Tutorial 1]()\n",
        "\n",
        "* Select the LLM API and model to use below. The default values are recommended, but more expensive models such as GPT-4-Turbo may give better results.\n",
        "\n",
        "* Select \"Run all\" from the Runtime menu above.\n",
        "\n",
        "* Enter your API key below when prompted\n",
        "\n",
        "* Proceed top-down following the instructions\n",
        "\n",
        "**New to Colab notebooks?**\n",
        "\n",
        "Colab notebooks are browser-based learning environments consisting of *cells* that include either text or code. The code is executed in a Google virtual machine instead of your own computer. You can run code cell-by-cell (click the \"play\" symbol of each code cell), and selecting \"Run all\" as instructed above is usually the first step to verify that everything works. For more info, see Google's [Intro video](https://www.youtube.com/watch?v=inN8seMm7UI) and [curated example notebooks](https://colab.google/notebooks/)\n",
        "\n"
      ],
      "metadata": {
        "id": "VDm0UCJ5NWP7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mobjxurDc9CH",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#Initial setup code. If you opened this notebook in Colab, this code is hidden\n",
        "#by default to avoid unnecessary user interface clutter\n",
        "\n",
        "#-------------------------------------------------------\n",
        "#User-defined parameters. You can freely edit the values\n",
        "llm_API=\"OpenAI\" # @param [\"OpenAI\", \"Aalto\"]\n",
        "LLM_model=\"gpt-4o\" #@param [\"gpt-4o-mini\",\"gpt-4o\", \"gpt-4-turbo\"]\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------\n",
        "#Implementation. Only edit this part if you know what your are doing\n",
        "\n",
        "#Import packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from IPython.display import HTML, clear_output\n",
        "import getpass\n",
        "import os\n",
        "import html\n",
        "import plotly.express as px\n",
        "import textwrap\n",
        "import openpyxl\n",
        "import re\n",
        "\n",
        "#determine if we are running in Colab\n",
        "import sys\n",
        "original_dir = os.getcwd()\n",
        "RunningInCOLAB = 'google.colab' in sys.modules\n",
        "if RunningInCOLAB:\n",
        "  import plotly.io as pio\n",
        "  pio.renderers.default = \"colab\"\n",
        "  if not os.path.exists(\"LLMCode\"):\n",
        "    if not os.getcwd().endswith(\"LLMCode\"):\n",
        "      print(\"Cloning the LLMCode repository...\")\n",
        "      #until the repo is public, we download this working copy instead of cloning\n",
        "      #(shared as: anyone with the link can view)\n",
        "      !wget \"https://drive.google.com/uc?export=download&id=1Td6ukrRGK9sUjlH1c6VTAYp2t_E1UNuQ\" -O LLMCode.zip\n",
        "      !mkdir LLMCode\n",
        "      !unzip -q LLMCode.zip -d LLMCode\n",
        "      #!git clone https://github.com/PerttuHamalainen/LLMCode.git\n",
        "  if not os.getcwd().endswith(\"LLMCode\"):\n",
        "    os.chdir(\"LLMCode\")\n",
        "    print(\"Installing dependencies...\")\n",
        "    !pip install -r requirements_notebooks.txt\n",
        "import llmcode\n",
        "os.chdir(original_dir)\n",
        "\n",
        "#Jupyter is already running an asyncio event loop => need this hack for async OpenAI API calling\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "#Prompt the user for an API key if not provided via a system variable\n",
        "if llm_API==\"OpenAI\":\n",
        "    if os.environ.get(\"OPENAI_API_KEY\") is None:\n",
        "        print(\"Please input an OpenAI API key\")\n",
        "        api_key = getpass.getpass()\n",
        "        os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "elif llm_API==\"Aalto\":\n",
        "    if os.environ.get(\"AALTO_OPENAI_API_KEY\") is None:\n",
        "        print(\"Please input an Aalto OpenAI API key\")\n",
        "        api_key = getpass.getpass()\n",
        "        os.environ[\"AALTO_OPENAI_API_KEY\"] = api_key\n",
        "else:\n",
        "    print(f\"Invalid API type: {llm_API}\")\n",
        "\n",
        "#Initialize the LLMCode library\n",
        "llmcode.init(API=llm_API)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XXvYWAWI2J1"
      },
      "source": [
        "# Preliminaries\n",
        "First, let's have some quick tests/demonstrations about how we can prompt an LLM using Python code."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompting a LLM using Python\n",
        "Prompting a LLM is straighforward, as shown below. Note that **the lines starting with \"#\" are not code.** Instead, they are comments that describe with the code below them does.\n",
        "\n",
        "If you want to learn more about Python basics such as variables and functions, check out this [YouTube playlist](https://www.youtube.com/playlist?list=PLUaB-1hjhk8GHKfndKjyDMHPg_HlQ4vpK)."
      ],
      "metadata": {
        "id": "jGFmI2hvna6M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PEN69G4I2J2"
      },
      "outputs": [],
      "source": [
        "#Define the prompt and store it in a variable (a container for some data)\n",
        "#called \"my_prompt\".\n",
        "my_prompt=\"Hi!\"\n",
        "\n",
        "#Call the query_LLM() function from the LLMCode library.\n",
        "#Functions are pieces of Python code that perform some functionality.\n",
        "#Here, the query_LLM() function takes in the \"prompts\" and \"model\" parameters and\n",
        "#and sends the prompts to the LLM. The \"LLM_model\" is the model you defined above.\n",
        "#The LLM response is is stored in the \"response\" variable\"\n",
        "response = llmcode.query_LLM(prompts=my_prompt,\n",
        "                             model=LLM_model)\n",
        "\n",
        "#Print out the response.\n",
        "print(\"LLM response:\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iC6-amAI2J5"
      },
      "source": [
        "### Prompting a LLM to highlight or extract relevant text\n",
        "\n",
        "Let's first test a simple prompt for producing highlights relating to a research question.\n",
        "\n",
        "**Exercise:**\n",
        "\n",
        "Modify the prompt to test the highlighting with some other description from the [Games as Art data](https://github.com/PerttuHamalainen/LLMCode/blob/master/test_data/bopp_test.csv).\n",
        "\n",
        "Here's one example that you can use:\n",
        "\n",
        "*Aside from the countless vistas the game provides, there is a moment at the very ended that affected me so profoundly that it couldn't think of any better way to explain it as art. You have spent the entire game nurturing this relationship with Delilah, as well as trying to handle Henry's own trauma. So much of their conversations reflecated how I felt, handled things. I wanted there to be a happy ending, a way in which both characters step away contented. But in truth, the ending is almost hallow, a real gut punch. I remember sitting as the credits role, wishing I could go back or somehow change the events, but they needed to be as they were to be that impactful.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4BDnUEwI2J6"
      },
      "outputs": [],
      "source": [
        "#Define the prompt.\n",
        "prompt=\"\"\"\n",
        "Below, I will give you a game experience description from a research experiment about experiencing video games as art. Your task is to assist in analyzing the experience description.\n",
        "\n",
        "The research question is: What feelings, emotions and sensations do players feel when experiencing video games as art?\n",
        "\n",
        "Please carry out the following task:\n",
        "- Identify and highlight statements relevant to the research question.\n",
        "\n",
        "- Respond by repeating the original text, but surrounding the statements with double asterisks (**), as if they were bolded text in a Markdown document.\n",
        "\n",
        "Below, I first give you an example of the output you should produce given the input.\n",
        "\n",
        "After that, I give you the actual input to process.\n",
        "\n",
        "\n",
        "EXAMPLE INPUT:\n",
        "\n",
        "the experience was one of curiosity, uncertainty and puzzeling. i felt calmness, clarity and beauty. at first i did not know what to expect exactly, but with time, i learned that this was part of the experience. moving around pieces of art made me feel as if i am part of the art, i felt, the art was part of the interaction: everything came to live through my interaction, which made me feel part of it.\n",
        "\n",
        "EXAMPLE OUTPUT:\n",
        "\n",
        "**the experience was one of curiosity, uncertainty and puzzeling. i felt calmness, clarity and beauty.** at first i did not know what to expect exactly, but with time, i learned that this was part of the experience. **moving around pieces of art made me feel as if i am part of the art, i felt, the art was part of the interaction: everything came to live through my interaction, which made me feel part of it.**\n",
        "\n",
        "ACTUAL INPUT:\n",
        "\n",
        "The Shapeshifting Detective is a supernatural murder mystery game with three potential culprits, one of whom is the tarot reader Rayne. Possessed by an interdimensional being known as a traveller, Rayne is trying to cover up the murder the traveller forced him to commit. If the player doesn't have Rayne jailed, the game concludes with Rayne kidnapping you, planning to murder you so you cannot have him put in prison for the traveller's crime. In response, you can shapeshift into his closest friend, Bronwyn, to his shock. Resigned, he tells you he has no choice but to leave, in essence exiling himself so the traveller cannot return and he is not imprisoned. With a sad smile, he says \"I'll miss you the most\" and walks away, never to be seen again. I found this scene to be incredibly touching; Rayne is driven by his self-preservation and terror, yet is simultaneously unable to hurt Bronwyn even if refusing to do so puts him at risk. He cuts himself off from who he cares about most to protect them, and I think there's a beauty in that tragedy.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "#Query the LLM\n",
        "response=llmcode.query_LLM(prompt,model=LLM_model)\n",
        "\n",
        "#Print the LLM output formatted so that the highlights are in bold\n",
        "print(\"LLM output:\\n\")\n",
        "display(HTML(llmcode.extracts_to_html(response)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubyLWyCMrnLM"
      },
      "source": [
        "# Processing multiple texts\n",
        "\n",
        "The simple prompting approach above breaks down if you have many texts to process. You can of course just copy-paste a whole document to the prompt, but the longer the document, the more likely it is that the LLM makes errors.\n",
        "\n",
        "To mitigate the above, LLMCode provides the following:\n",
        "- A simple interface for processing the data in multiple chunks\n",
        "- Automatic checking for LLM hallucinations - one wants to be sure that the LLM outputs the correct text instead of omitting anything or inventing new text.\n",
        "- Automatic correcting of minor hallucinations - for instance, it is quite common that the LLM automatically corrects spelling of the highlighted text. LLMCode detects that and resubstitutes the original text.\n",
        "\n",
        "To use LLMCode to highlight relevant passages, you need to:\n",
        "- Define the prompt beginning that describes the data and research question\n",
        "- Define a number of examples\n",
        "\n",
        "The cells below show you how."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load data\n",
        "Running the code below loads test data and prints a number of highlighted examples.\n",
        "\n",
        "By default, this notebook uses the [Games As Art](https://osf.io/ryvt6/) open dataset from a survey about how and why people experience video games as art. For testing this notebook, we have annotated the freeform artistic game experience descriptions by highlighting parts of that describe feelings, emotions, and sensations experienced by the player.\n",
        "\n",
        "**User-Defined Parameters**\n",
        "\n",
        "*data_filename_or_URL* : Filename or URL of the data to analyze. This notebook supports either spreadsheets (.csv or .xlsx) with texts in a single column or Word (.docx) with highlighted texts marked with the commenting functionality. Note that for .docx, URLs are not currently supported.\n",
        "\n",
        "*examples_filename_or_URL* : Filename or URL of examples of what to highlight.  If this is empty, it is assumed that the analyzed data also contains example annotations. This is the case with our default test data, but the option to use a separate example file is provided in case one wants to process multiple data files using the same examples. Note that for .docx, URLs are not currently supported.\n",
        "\n",
        "*data_column*: The name of the column containing the analyzed text. For .docx files, don't change the defaults.\n",
        "\n",
        "*ground_truth_column*: The name of the column containing ground-truth human highlights. For .docx files, don't change the defaults.\n",
        "\n",
        "*validation_data* : How many first texts of the dataset to use for so-called validation data (explained later). A smaller value makes working with this tutorial faster and cheaper but produces less reliable quality metrics. We recommend testing the notebook with the defaults.\n",
        "\n",
        "*test_data* : How many first texts of the dataset after the validation data to use for so-called test data (explained later). A smaller value makes working with this tutorial faster and cheaper but produces less reliable quality metrics. We recommend testing the notebook with the defaults.\n",
        "\n",
        "*examples_to_view* : How many examples to print out\n",
        "\n",
        "\n",
        "**How to use your own data?**\n",
        "\n",
        "If you have your own data as a column of texts in an Excel or csv file, you can either 1) upload it to Colab using the file browser on the left and input its filename, or 2) input a download URL for your data. Remember to specify which data columns to use!"
      ],
      "metadata": {
        "id": "9j1IHe84tvOg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hytkWKhdL7k",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#-------------------------------------------------------\n",
        "#User-defined parameters. You can freely edit the values\n",
        "data_filename_or_URL=\"LLMCode/test_data/bopp_test_augmented_feelings2.docx\" #@param {type:\"string\"}\n",
        "examples_filename_or_URL=\"\" #@param {type:\"string\", placeholder:\"leave this empty to use examples from the data file\"}\n",
        "data_column=\"text\" #@param {type:\"string\"}\n",
        "ground_truth_column=\"coded_text\" #@param {type:\"string\"}\n",
        "validation_data=50 #@param {type:\"integer\"}\n",
        "test_data=50 #@param {type:\"integer\"}\n",
        "examples_to_view=10 #@param {type:\"integer\"}\n",
        "\n",
        "#-------------------------------------------------------------------\n",
        "#Implementation. Only edit this part if you know what your are doing\n",
        "\n",
        "#data load helper function\n",
        "def load_data(filename_or_URL):\n",
        "  #Load the file\n",
        "  if filename_or_URL.endswith(\".xlsx\"):\n",
        "    df = pd.read_excel(filename_or_URL)\n",
        "  elif filename_or_URL.endswith(\".docx\"):\n",
        "    df = llmcode.open_docx_and_process_codes(filename_or_URL)\n",
        "  elif filename_or_URL.endswith(\".csv\"):\n",
        "    df = pd.read_csv(filename_or_URL)\n",
        "  else:\n",
        "    raise Exception(\"File type not supported.\")\n",
        "\n",
        "  #Fix a possible Excel import issue\n",
        "  df[data_column]=df[data_column].astype(str).apply(openpyxl.utils.escape.unescape)\n",
        "  if ground_truth_column in df.columns:\n",
        "    df[ground_truth_column]=df[ground_truth_column].astype(str).apply(openpyxl.utils.escape.unescape)\n",
        "\n",
        "  #In this notebook, we only focus on the highlights\n",
        "  #Thus, we remove any codes defined for the highlights enclosed between <sup> and </sup>\n",
        "  if ground_truth_column in df.columns:\n",
        "    df[ground_truth_column]=df[ground_truth_column].str.replace(r'<sup>.*?</sup>', '', regex=True)\n",
        "  if ground_truth_column in df_examples.columns:\n",
        "    df_examples[ground_truth_column]=df_examples[ground_truth_column].str.replace(r'<sup>.*?</sup>', '', regex=True)\n",
        "  return df\n",
        "\n",
        "#load data file\n",
        "df=load_data(data_filename_or_URL)\n",
        "\n",
        "#do the validation-test data split\n",
        "df_test=df.iloc[validation_data:validation_data+test_data]\n",
        "df=df.head(validation_data)\n",
        "\n",
        "#load example file if defined. if not, we take a copy of the validation data\n",
        "if examples_filename_or_URL:\n",
        "  df_examples=load_data(examples_filename_or_URL)\n",
        "  df_examples=df_examples.head(rows_to_use)\n",
        "else:\n",
        "  df_examples=df.copy()\n",
        "\n",
        "#Print examples formatted so that the highlights are in bold\n",
        "print(f\"{examples_to_view} first rows of the example data:\")\n",
        "html_text=llmcode.extracts_to_html(df_examples.head(examples_to_view)[ground_truth_column])\n",
        "display(HTML(html_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Processing the data\n",
        "\n",
        "The code below does the following:\n",
        "- Process the loaded data\n",
        "- If the data contains ground truth human highlights, calculate human-LLM agreement calculates their overlap using **Intersection over Union (IoU, a.k.a. Jaccard Index)** in range 0...1, where 0 means no overlap and 1 means perfect overlap, i.e., identical human and LLM highlights. More info on IoU: https://en.wikipedia.org/wiki/Jaccard_index.\n",
        "- If IoU was calculated, we print it along with a table of human and LLM highlights side-by-side, sorted by IoU.\n",
        "\n",
        "More info on IoU: https://en.wikipedia.org/wiki/Jaccard_index.\n",
        "\n",
        "\n",
        "**How to examine the output**\n",
        "\n",
        "To get an idea of the worst-case errors made by the LLM, check out the last rows of the output table which have the lowest IoU values.\n",
        "\n",
        "**Exercise 1**\n",
        "\n",
        "Increase the number of examples used in the prompt using the provided slider. How do the results change?\n",
        "\n",
        "**Exercise 2**\n",
        "\n",
        "Inspect the results table. Do the rows with lowest IoU reveal errors or inconsistencies in the human-annotated ground truth highlights? This is more common than one might think. Although qualitative data analysis can be subjective and it may not make sense to compare the codes and highlights of two human coders, we have found the human-LLM comparison to be revealing one's own inconsistencies.\n",
        "\n",
        "**Exercise 3**\n",
        "\n",
        "Click to show the code and try to edit the prompt to further improve the results. Can you add more instructions or change the wording of the prompt to be more direct and unambiguous? Remember that a good LLM prompt always provides 1) precise instructions and 2) enough high-quality examples.\n",
        "\n",
        "* A model solution to Exercise 3 is given in the cell below.*\n"
      ],
      "metadata": {
        "id": "6BFWK6RS0kxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------\n",
        "#User-defined parameters. You can freely edit the values\n",
        "\n",
        "#Number of examples to use from the example data\n",
        "#Note that the numbering is 0-based, i.e.,\n",
        "number_of_examples=2 #@param {type:\"slider\",min:\"1\",max:\"10\"}\n",
        "\n",
        "#Define the prompt beginning. The code below will automatically add the examples.\n",
        "prompt=\"\"\"Below, I will give you a game experience description from a research experiment about experiencing video games as art. Your task is to assist in analyzing the experience description.\n",
        "\n",
        "The research question is: What feelings, emotions and sensations do players feel when experiencing video games as art?\n",
        "\n",
        "Please carry out the following task:\n",
        "- Identify and highlight statements relevant to the research question.\n",
        "\n",
        "- Respond by repeating the original text, but surrounding the statements with double asterisks (**), as if they were bolded text in a Markdown document.\n",
        "\"\"\"\n",
        "\n",
        "#-------------------------------------------------------------------\n",
        "#Implementation. Only edit this part if you know what your are doing\n",
        "\n",
        "#Add the examples to the prompt\n",
        "prompt+=\"\"\"\n",
        "\n",
        "Below, I first give you an example of the output you should produce given the input.\n",
        "\n",
        "After that, I give you the actual input to process.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "for example in range(number_of_examples):\n",
        "  prompt+=f\"EXAMPLE INPUT:\\n\\n{df_examples.iloc[example][data_column]}\\n\\n\"\n",
        "  prompt+=f\"EXAMPLE OUTPUT:\\n\\n{df_examples.iloc[example][ground_truth_column]}\\n\\n\"\n",
        "prompt+=\"ACTUAL INPUT:\\n\\n\"\n",
        "\n",
        "\n",
        "#call the extract_relevant method with the prompt and data\n",
        "df_extracts=llmcode.extract_relevant(prompt,\n",
        "                          df,\n",
        "                          data_col=data_column,\n",
        "                          extracts_col=\"llm_extracts\"\n",
        "                         )\n",
        "\n",
        "#calculate the IoU\n",
        "IoU,html_report=llmcode.extract_IoU(df_extracts,\n",
        "                                    extracts_col=\"llm_extracts\",\n",
        "                                    reference_col=ground_truth_column)\n",
        "\n",
        "#display the quality report and print out the average IoU\n",
        "display(HTML(html_report))\n",
        "print(f\"Average IoU = {np.mean(IoU)}\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "YkE6Qus71jwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution to Exercise 3 (Click > to expand)"
      ],
      "metadata": {
        "id": "l02HbkdRGLLk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below is the same as above but the prompt has more explicit bullet-point instructions that address some of the common LLM errors.\n",
        "\n",
        "To further improve the IoU, one could:\n",
        "- Correct the inconsistencies in the ground truth data. For instance, the human labeler has missed the statement *The experiences with the game's characters up to that point (in overall plot scenes, single-character support dialogues, and even small in-battle dialogue), overarching plot, and even visual/music cues made it feel like a very significant decisive moment.* Furthermore, experiencing beauty is something that repeats in the data and it might also be considered a feeling. Therefore, passages such as \"I think there's a beauty in that tragedy.\" should maybe be highlighted, as the LLM highlights suggest.\n",
        "- Add some of the worst-case results to the examples. However, note the closing remarks below on overfitting."
      ],
      "metadata": {
        "id": "CyMZj_yDGQ7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------\n",
        "#User-defined parameters. You can freely edit the values\n",
        "\n",
        "#Number of examples to use from the example data\n",
        "#Note that the numbering is 0-based, i.e.,\n",
        "num_examples=10\n",
        "\n",
        "#Define the prompt beginning. The code below will automatically add the examples.\n",
        "improved_prompt=\"\"\"Below, I will give you a game experience description from a research experiment about experiencing video games as art. Your task is to assist in analyzing the experience description.\n",
        "\n",
        "The research question is: What feelings, emotions and sensations do players feel when experiencing video games as art?\n",
        "\n",
        "Please carry out the following task:\n",
        "- Identify and highlight statements relevant to the research question.\n",
        "\n",
        "- Respond by repeating the original text, but highlighting the relevant statements by surrounding the statements with double asterisks, as if they were bolded text in a Markdown document.\n",
        "\n",
        "- Ignore other text, e.g., text that talks about what art is in general or only describes the game but not aspects of the player's subjective experience such as what they feel or think. However, note that a description of a game can indirectly describe the experience; for example, \"The game combined enchanting graphics with a calming soundtrack\" indicates the player feeling enchanted and calm.\n",
        "\n",
        "- Preserve exact formatting of the original text. Do not correct typos or remove unnecessary spaces.\n",
        "\n",
        "- If no statements were found, just respond with the original text\n",
        "\"\"\"\n",
        "\n",
        "#-------------------------------------------------------------------\n",
        "#Implementation. Only edit this part if you know what your are doing\n",
        "\n",
        "#Add the examples to the prompt\n",
        "improved_prompt+=\"\"\"\n",
        "\n",
        "Below, I first give you an example of the output you should produce given the input.\n",
        "\n",
        "After that, I give you the actual input to process.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "for example in range(num_examples):\n",
        "  improved_prompt+=f\"EXAMPLE INPUT:\\n\\n{df_examples.iloc[example][data_column]}\\n\\n\"\n",
        "  improved_prompt+=f\"EXAMPLE OUTPUT:\\n\\n{df_examples.iloc[example][ground_truth_column]}\\n\\n\"\n",
        "improved_prompt+=\"ACTUAL INPUT:\\n\\n\"\n",
        "\n",
        "\n",
        "#call the extract_relevant method with the prompt and data\n",
        "df_extracts=llmcode.extract_relevant(improved_prompt,\n",
        "                          df,\n",
        "                          data_col=data_column,\n",
        "                          extracts_col=\"llm_extracts\"\n",
        "                         )\n",
        "\n",
        "#calculate the IoU\n",
        "IoU,html_report=llmcode.extract_IoU(df_extracts,\n",
        "                                    extracts_col=\"llm_extracts\",\n",
        "                                    reference_col=ground_truth_column)\n",
        "\n",
        "#display the quality report and print out the average IoU\n",
        "display(HTML(html_report))\n",
        "print(f\"Average IoU = {np.mean(IoU)}\")\n"
      ],
      "metadata": {
        "id": "YKzSv-WFFx6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exporting the results for further analysis\n",
        "A typical use case for the LLM-based highlighting is to use it as a preprocessing step for manual qualitative coding, helping the coder to quickly spot the relevant parts of the text. However, you should only do this if the performance of the model is acceptable&mdash;**are you ok with the frequency and types of errors the LLM makes with your test data?**\n",
        "\n",
        "To code the LLM-highlighted data in a tool such as Atlas.ti, you can run the code below to export it as .pdf. If you run this notebook locally, the .pdf is saved to your computer. If you run this notebook in Colab, the .pdf can be downloaded using Colab's file browser (click the \"folder\" icon on the left).\n"
      ],
      "metadata": {
        "id": "EMYGnXL-XMhF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------\n",
        "#User-defined parameters. You can freely edit the values\n",
        "\n",
        "pdf_filename = \"LLM_highlights.pdf\"  #@param {type:\"string\"}\n",
        "\n",
        "#-------------------------------------------------------------------\n",
        "#Implementation. Only edit this part if you know what your are doing\n",
        "markdown_output=\"\\n\\n\".join(df_extracts[\"llm_extracts\"])\n",
        "from markdown_pdf import MarkdownPdf, Section\n",
        "pdf = MarkdownPdf(toc_level=2)\n",
        "pdf.add_section(Section(markdown_output))\n",
        "pdf.save(pdf_filename)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "JuTWlhF0YQte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Closing remarks: Validation and Test Data\n",
        "In all AI and Machine Learning, a common danger is to [overfit](https://en.wikipedia.org/wiki/Overfitting) one's model or approach to some data, making it generalize poorly to new data.\n",
        "\n",
        "**The more you iterate on your prompt instructions and examples, the more you are in danger of overfitting.**\n",
        "\n",
        "This is why it is a standard practice to split one's data into [three distinct parts](https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets):\n",
        "\n",
        "1. Training data: This is used to train a model. When using readymade LLM's, one does not have access to the training data, and we can ignore this concept here.\n",
        "\n",
        "2. Validation data: This is typically used to search for the best possible [hyperparameters](https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning) such as when to stop training or how many layers to use in a neural network model.\n",
        "\n",
        "3. Test data: This is used to test the performance of the final model after the hyperparameter tuning. *Separating test and validation data avoids overly optimistic test results caused by overfitting the hyperparameters to the validation data*.\n",
        "\n",
        "The prompt instructions and examples can be considered as hyperparameters. Therefore, **one should ideally iterate/optimize the prompt with validation data and when done, verify the performance with separate test data**. This is especially important if your human-defined reference dataset is small.\n",
        "\n",
        "**For academic research, we recommend using at least 100 texts for both the validation and test data**, i.e., the data file should have at least 200 texts with human-annotated ground truth highlights, as the first 100 would be used for validation and next 100 for testing.\n",
        "\n",
        "For industry research, the designer or researcher should use their own judgement - how crucial is it to be able to measure the performance accurately?\n",
        "\n",
        "**How to report LLM use in research papers?** There does not currently exist an established best practice for reporting LLM-based qualitative analysis tool use, but if you use the LLM-based highlighting, you could report at least the full prompt with examples, the number of validation and test data texts, the validation and test data IoUs, and a table with examples of the worst and best case LLM performance so that the reader can judge themselves if the LLM performance is acceptable.\n",
        "\n",
        "**Exercise: Process the test data and calculate IoU**\n",
        "\n",
        "Run the code below to calculate the IoU using the test data specified earlier. Note: this will re-use the prompt defined in the \"Processing the data\" section above. Edit and run that part again if you want to re-test with the test data.\n",
        "\n",
        "Inspect the results. Is the LLM performance different than for the validation data? Can you spot any further human annotation errors or inconsistencies that should perhaps be corrected?\n",
        "\n"
      ],
      "metadata": {
        "id": "bIUXkrg3-kCG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "#call the extract_relevant method with the prompt and data\n",
        "df_extracts_test=llmcode.extract_relevant(prompt,\n",
        "                          df_test,\n",
        "                          data_col=data_column,\n",
        "                          extracts_col=\"llm_extracts\"\n",
        "                         )\n",
        "\n",
        "#calculate the IoU\n",
        "IoU,html_report=llmcode.extract_IoU(df_extracts_test,\n",
        "                                    extracts_col=\"llm_extracts\",\n",
        "                                    reference_col=ground_truth_column)\n",
        "\n",
        "#display the quality report and print out the average IoU\n",
        "display(HTML(html_report))\n",
        "print(f\"Average IoU = {np.mean(IoU)}\")\n"
      ],
      "metadata": {
        "id": "Sjw5rcbOEaol",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "l02HbkdRGLLk"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}