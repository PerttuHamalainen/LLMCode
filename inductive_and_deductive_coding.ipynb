{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa2d08c-b25f-48d5-9a86-cdae374d306c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import llmcode\n",
    "from IPython.display import HTML, Markdown, display\n",
    "import getpass\n",
    "import os\n",
    "import html\n",
    "import lxml\n",
    "import re\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import textwrap\n",
    "import random\n",
    "import math\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50fe3b8-5f44-42c7-8f4a-17efaf2de4fc",
   "metadata": {},
   "source": [
    "Init the LLMCode library. Set the llm_API variable to \"Aalto\" to use Aalto's GDPR-safe Azure OpenAI API endpoints that are suitable for processing confidential data. Here, we use the OpenAI API because it is faster and makes this notebook usable for people outside Aalto.\n",
    "\n",
    "When you run the code, it will ask you to input an appropriate API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb65db2f-60bf-4da9-b008-58a99e2b72e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_API=\"OpenAI\"\n",
    "if llm_API==\"OpenAI\":\n",
    "    if os.environ.get(\"OPENAI_API_KEY\") is None:\n",
    "        print(\"Please input an OpenAI API key\")\n",
    "        api_key = getpass.getpass()\n",
    "        os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "elif llm_API==\"Aalto\":\n",
    "    if os.environ.get(\"AALTO_OPENAI_API_KEY\") is None:\n",
    "        print(\"Please input an Aalto OpenAI API key\")\n",
    "        api_key = getpass.getpass()\n",
    "        os.environ[\"AALTO_OPENAI_API_KEY\"] = api_key\n",
    "else:\n",
    "    print(f\"Invalid API type: {llm_API}\")\n",
    "llmcode.init(API=llm_API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988666e2-1b2b-41e6-b550-9b01efa2ce68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter is already running an asyncio event loop => need this hack for async OpenAI API calling\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bda621-5ba9-4473-b727-a36ff84e3e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the GPT model to use\n",
    "gpt_model = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2151756-7d4d-4c2c-9c61-d37796d74477",
   "metadata": {},
   "source": [
    "# Coding with LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54b2305-f007-47d0-b9d8-9a0f71bcb325",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e5955a-55b2-44f6-b676-e84443f202c3",
   "metadata": {},
   "source": [
    "Below, first write the research question you would like to answer by analysing your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474de957-3b81-49e1-959d-5d05cd2cb9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "research_question = \"How do people experience games as art?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085cb592-98b2-4fdf-b0aa-a1798f4c15de",
   "metadata": {},
   "source": [
    "Open a Word file containing texts separated by \"-----\" (five dashes) that are human-coded using comments. You may either provide a path to the file or leave file_path to None, in which case the system will prompt you to select the file yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4bc8c8-95c9-4f65-aa7b-4dd82f28e4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"test_data/bopp_test_augmented_joel.docx\"\n",
    "df = llmcode.open_docx_and_process_codes(file_path)\n",
    "\n",
    "# Limit dataset to last coded instance, in case dataset is not fully coded\n",
    "def contains_coded_pattern(text):\n",
    "    return bool(re.search(r\"\\*\\*.*?<sup>.*?</sup>\", text))\n",
    "df['is_coded'] = df['coded_text'].apply(contains_coded_pattern)\n",
    "last_coded_index = df[df['is_coded']].index.max()\n",
    "df = df.loc[:last_coded_index].drop(columns=['is_coded'])\n",
    "print(f\"Using {len(df)} coded instances\")\n",
    "\n",
    "# Remove leading and trailing whitespace in dataset to simplify analysis\n",
    "df['text'] = df['text'].str.strip()\n",
    "df['coded_text'] = df['coded_text'].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ea6161-7447-4279-baca-789c8fef9bcc",
   "metadata": {},
   "source": [
    "Let's inspect a sample of the texts, which are annotated using the Markdown format \\*\\*highlight\\*\\*\\<sup>codes separated by a semicolon\\</sup>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ece95a7-14ad-436c-a0f7-716f33c55c37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_samples = 5\n",
    "for _, row in df.sample(n_samples).iterrows():\n",
    "    display(Markdown(row.coded_text.replace(\"\\n\", \"<br/>\")))  # Render newlines correctly\n",
    "    display(Markdown(\"---\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f9cc2b-b519-460e-b338-8dd0640e21cd",
   "metadata": {},
   "source": [
    "Let's inspect the codes in the human-annotated texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814966fa-55b6-4a4b-9848-9ac68e4f8d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_all_codes(df):\n",
    "    codes = [code for coded_text in df.coded_text for _, code in llmcode.parse_codes(coded_text)]\n",
    "    code_counts = Counter(codes)\n",
    "    for code in sorted(code_counts):\n",
    "        print(f\"{code} ({code_counts[code]})\")\n",
    "\n",
    "print(\"\\nHUMAN-ANNOTATED CODES:\\n\")\n",
    "print_all_codes(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359c46f0-b04a-4b98-896c-39b545a22198",
   "metadata": {},
   "source": [
    "## Basic example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f72f1c-9574-4f46-b0bd-de6fac787cac",
   "metadata": {},
   "source": [
    "Feel free to investigate how changes to the prompt affect the LLM output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45d2458-b497-4cc0-ab92-4f8ff5e53a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "prompt = \"\"\"I will give you a game experience description from a qualitative research experiment about experiencing video games as art. \n",
    "\n",
    "Please carry out the following task:\n",
    "- Identify and code statements about the subjective experience, if there are any. \n",
    "- Respond by repeating the original text, but highlighting the coded statements by surrounding the statements with double asterisks, as if they were bolded text in a Markdown document.\n",
    "- Include the associated code(s) immediately after the statement, separated by a semicolon and enclosed in <sup></sup> tags, as if they were superscript text in a Markdown document.\n",
    "- Ignore other text, e.g., text that only describes the game but not the player's subjective experience.\n",
    "- Preserve exact formatting of the original text. Do not correct typos or remove unnecessary spaces.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bde17ad-c57b-4cf1-9041-d7ec1016e874",
   "metadata": {},
   "source": [
    "## Inductive coding using few-shot examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c604a44-cd0b-42cd-a606-5348fe455dc5",
   "metadata": {},
   "source": [
    "Below, we first choose a couple of few-shot examples that we use to teach the LLM our coding style. Using a prompt and these examples, we instruct it to code the rest of the dataset. Finally, we compare the LLM-coded texts to human-coded ones.\n",
    "\n",
    "We use the code_inductively() function from the LLMCode package, which codes a list of texts given a research question and a DataFrame containing few-shot examples. The function prompts the LLM with batches of the text, which is faster than only prompting one text at a time. The function also attempts to correct some common errors that the LLM may make, such as correcting typos or omitting some non-coded sentences of the original text. For further analyses, we want the LLM to preserve the exact formatting of the original text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f38cc2a-469c-4457-9fc1-b34a693cd8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of few-shot examples\n",
    "n_examples = 8\n",
    "\n",
    "# Ensure that few-shot examples are excluded from the input texts\n",
    "few_shot_examples = df.sample(n=n_examples, random_state=1)\n",
    "df_input = df.drop(few_shot_examples.index).reset_index(drop=True)\n",
    "few_shot_examples = few_shot_examples.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1249fa77-1bec-4109-82d5-76f2b7f0e107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inductive coding\n",
    "coded_texts_ind = llmcode.code_inductively(\n",
    "    texts=df_input.text.tolist(),\n",
    "    research_question=research_question,\n",
    "    few_shot_examples=few_shot_examples,\n",
    "    gpt_model=gpt_model\n",
    ")\n",
    "\n",
    "# Compare a sample of LLM codes to human codes\n",
    "def print_code_sample(df_input, llm_coded_texts, n):\n",
    "    random_sample = random.sample(list(zip(llm_coded_texts, df_input.coded_text.tolist())), n)\n",
    "    for llm_coded, human_coded in random_sample:\n",
    "        print(\"\\nLLM CODED:\")\n",
    "        display(Markdown(llm_coded))\n",
    "        print(\"\\nHUMAN CODED:\")\n",
    "        display(Markdown(human_coded))\n",
    "        display(Markdown(\"---\"))\n",
    "\n",
    "print_code_sample(df_input, coded_texts_ind, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0308f36-e692-4c5f-8801-de7776a0aeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all codes in few-shot examples\n",
    "print(\"\\nCODES IN FEW SHOT EXAMPLES:\\n\")\n",
    "print_all_codes(few_shot_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed552b41-59c4-430c-946c-562bf2429015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse all codes and highlights in LLM output\n",
    "code_highlights_ind = llmcode.get_codes_and_highlights(coded_texts_ind)\n",
    "\n",
    "# Print a list of all LLM-generated codes\n",
    "print(\"\\nALL LLM-GENERATED CODES AND THEIR COUNTS:\\n\")\n",
    "for code, highlights in sorted(code_highlights_ind.items()):\n",
    "    print(f\"{code} ({len(highlights)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f593a27e-e90e-47eb-a79f-d71544421535",
   "metadata": {},
   "source": [
    "Let's visualise the LLM codes with the help of word embeddings, which capture the meaning of words and can therefore be used to explore code similarities. Word embeddings are typically high dimensional vectors, so we will reduce the dimensionality to 2 in order to plot them in 2d. We create an interactive visualisation of the code embeddings using Plotly, so that you can hover over the plot to reveal the code name and an example of an associated highlight from the texts. The size of each marker corresponds to the number of annotations for that code in the text. Can you tell a difference between the counts of the codes included in the few-shot examples (in blue) and codes generated by the LLM (in red)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29529259-2034-4c57-bfbd-ea6997ccbf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse all codes with respective highlights in human-annotated input texts, for comparison\n",
    "human_code_highlights = llmcode.get_codes_and_highlights(df_input.coded_text)\n",
    "\n",
    "# Prepare list of codes in the few-shot examples\n",
    "few_shot_codes = set(code for coded_text in few_shot_examples.coded_text for _, code in llmcode.parse_codes(coded_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b07acaf-3bb4-4193-8af6-be0253eae09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A context string helps the embedding model disambiguate code labels in this specific context\n",
    "embedding_context = f\", in the context of the research question: {research_question}\"\n",
    "embedding_model = \"text-embedding-3-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b65e38-2bde-4a2b-b5cf-1301c10779a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_code_vis_df(code_highlights, human_code_highlights, few_shot_codes, embedding_context, embedding_model):\n",
    "    # Find code embeddings for all codes\n",
    "    all_codes = set(code_highlights.keys()).union(set(human_code_highlights.keys()))\n",
    "    df_em = llmcode.get_2d_code_embeddings(list(all_codes), embedding_context, embedding_model)\n",
    "    \n",
    "    # Create DataFrame of LLM-generated codes\n",
    "    df_llm = pd.DataFrame([(c,) for c in code_highlights.keys()], columns=[\"code\"])\n",
    "    df_llm[\"code_count\"] = df_llm[\"code\"].apply(lambda code: len(code_highlights[code]))\n",
    "    df_llm[\"example\"] = df_llm[\"code\"].apply(lambda code: code_highlights[code][0])\n",
    "    df_llm[\"group\"] = df_llm.code.apply(lambda code: \"LLM code (few-shot)\" if code in few_shot_codes else \"LLM code\")\n",
    "    \n",
    "    # Create DataFrame of human-generated codes\n",
    "    df_human = pd.DataFrame([(c,) for c in human_code_highlights.keys()], columns=[\"code\"])\n",
    "    df_human[\"code_count\"] = df_human[\"code\"].apply(lambda code: len(human_code_highlights[code]))\n",
    "    df_human[\"example\"] = df_human[\"code\"].apply(lambda code: human_code_highlights[code][0])\n",
    "    df_human[\"group\"] = \"Human code\"\n",
    "    \n",
    "    # Concatenate code DataFrames and merge with embeddings\n",
    "    df_em_codes = pd.concat([df_llm, df_human])\n",
    "    df_em_codes = df_em_codes.merge(df_em, on=\"code\", validate=\"many_to_one\")\n",
    "    return df_em_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1093ca-93fa-4f0e-abac-247f87e02be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_2d_embeddings(df_em):\n",
    "    # Prepare labels for visualisation\n",
    "    hover_texts = []\n",
    "    colors = []  # List to store color categories\n",
    "    for _, row in df_em.iterrows():\n",
    "        text = f\"{row.code} ({row.code_count})</br></br>\"\n",
    "\n",
    "        # Add an example of a code highlight\n",
    "        text += '\"' + \"</br>\".join(textwrap.wrap(row.example, width=60)) + '\"'\n",
    "        \n",
    "        hover_texts.append(text)\n",
    "\n",
    "        # Determine color category based on group\n",
    "        colors.append(row.group)\n",
    "    \n",
    "    df_vis = pd.DataFrame()\n",
    "    df_vis[\"Hover\"] = hover_texts\n",
    "    df_vis[\"Size\"] = [c / df_em[\"code_count\"].max() for c in df_em[\"code_count\"]]\n",
    "    df_vis[\"x\"] = df_em[\"code_2d_0\"]\n",
    "    df_vis[\"y\"] = df_em[\"code_2d_1\"]\n",
    "    df_vis[\"Color\"] = colors\n",
    "\n",
    "    # Plot the codes in 2D\n",
    "    fig = px.scatter(df_vis,\n",
    "                     width=1000, height=800,\n",
    "                     x=\"x\",\n",
    "                     y=\"y\",\n",
    "                     size=\"Size\",\n",
    "                     color=\"Color\",  # Set color categories\n",
    "                     hover_name=\"Hover\",\n",
    "                     title=\"Codes Visualised in 2D\")\n",
    "\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c721be2-c0a0-4a36-a73d-2e9e88196f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_em = prepare_code_vis_df(\n",
    "    code_highlights_ind,\n",
    "    human_code_highlights,\n",
    "    few_shot_codes,\n",
    "    embedding_context,\n",
    "    embedding_model\n",
    ")\n",
    "visualise_2d_embeddings(df_em)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41df2cc1-f2ab-4149-9f7a-63a8ecda7d8a",
   "metadata": {},
   "source": [
    "## Inductive coding with code consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdcf7c2-aece-4ccd-8776-b2edc9075ec6",
   "metadata": {},
   "source": [
    "Below, we run a similar experiment to the above but with code consistency. With this setting, the texts are processed sequentially, to allow the reuse of codes between text instances instead of creating possibly redundant new and only slightly different codes for each text. The system does this by keeping track of a list of previous codes that is added as input to each prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788a724a-cf54-418c-8a18-79af5ad41f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inductive coding with code consistency\n",
    "coded_texts_ind_con, code_descriptions_ind_con = llmcode.code_inductively_with_code_consistency(\n",
    "    texts=df_input.text.tolist(),\n",
    "    research_question=research_question,\n",
    "    few_shot_examples=few_shot_examples,\n",
    "    gpt_model=gpt_model\n",
    ")\n",
    "\n",
    "# Compare LLM codes to human codes\n",
    "print_code_sample(df_input, coded_texts_ind_con, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f568fd9b-afe8-45ab-b8fa-6108ee9be1b4",
   "metadata": {},
   "source": [
    "Let's again print the list of all generated codes. The above function also generates a description for each code in order to prevent the generation of duplicate codes with the same meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d2c606-66b4-44b9-ab99-174e60ced447",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parse all codes and highlights in LLM output\n",
    "code_highlights_ind_con = llmcode.get_codes_and_highlights(coded_texts_ind_con)\n",
    "\n",
    "# Print all LLM-created codes\n",
    "print(\"\\nLLM-GENERATED CODES:\\n\")\n",
    "for code, highlights in sorted(code_highlights_ind_con.items()):\n",
    "    print(f\"{code} ({len(highlights)}): {code_descriptions_ind_con[code]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea8d30c-7baa-415d-81a4-f6864668fa4e",
   "metadata": {},
   "source": [
    "Let's visualise the codes again. Compare these codes to the codes in the previous example, for example in terms of:\n",
    "- Redundancy\n",
    "- Alignment with human codes and few-shot codes\n",
    "- Coverage of what you see as the most important topics\n",
    "- Creativity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6797526b-a9e1-46b0-948d-380f33b58df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_em = prepare_code_vis_df(\n",
    "    code_highlights_ind_con,\n",
    "    human_code_highlights,\n",
    "    few_shot_codes,\n",
    "    embedding_context,\n",
    "    embedding_model\n",
    ")\n",
    "visualise_2d_embeddings(df_em)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c1a78a-67b7-47c3-981c-b42da701473a",
   "metadata": {},
   "source": [
    "## Deductive coding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9cca79-17a8-4148-9b21-8acdc8861715",
   "metadata": {},
   "source": [
    "In this section, we’ll demonstrate how to apply deductive qualitative coding using a LLM. Deductive coding involves applying predefined codes to a dataset, in this case using human-annotated examples as our codebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d71a24a-97cd-4371-bdeb-f83f30a3d1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use all human-annotated codes as the codebook\n",
    "codebook = [(code,) for coded_text in df.coded_text for _, code in llmcode.parse_codes(coded_text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9d54e8-8802-47f1-9349-620c09caf1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deductive coding\n",
    "coded_texts_ded = llmcode.code_deductively(\n",
    "    texts=df_input.text.tolist(),\n",
    "    research_question=research_question,\n",
    "    codebook=codebook,\n",
    "    few_shot_examples=few_shot_examples,\n",
    "    gpt_model=gpt_model\n",
    ")\n",
    "\n",
    "# Compare LLM codes to human codes\n",
    "print_code_sample(df_input, coded_texts_ded, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b746be-2eee-415f-b35a-2b265f7afa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse all codes and highlights in LLM output\n",
    "code_highlights_ded = llmcode.get_codes_and_highlights(coded_texts_ded)\n",
    "\n",
    "# Print all LLM-created codes\n",
    "print(\"\\nLLM-GENERATED CODES:\\n\")\n",
    "for code, highlights in sorted(code_highlights_ded.items()):\n",
    "    print(f\"{code} ({len(highlights)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c99902-0c46-4971-8381-88687d54c9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_em = prepare_code_vis_df(\n",
    "    code_highlights_ded,\n",
    "    human_code_highlights,\n",
    "    [],\n",
    "    embedding_context,\n",
    "    embedding_model,\n",
    ")\n",
    "visualise_2d_embeddings(df_em)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741e030c-7cdd-4d3e-a3be-c3d9df8e4f63",
   "metadata": {},
   "source": [
    "# Evaluating LLM coding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc11a0f0-fdad-461f-b364-f093570a64dc",
   "metadata": {},
   "source": [
    "## Comparing similarity of LLM- and human-generated codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6359ffdb-46a6-41b6-acd0-7c5aff1b77a4",
   "metadata": {},
   "source": [
    "Next, we investigate the similarity between LLM and human-generated codes. We will be comparing the results for each of the three approaches to LLM coding carried out above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56cf626-a52d-4f0f-9ecc-6b98a65bfef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "coded_texts_by_method = {\n",
    "    \"Inductive\": coded_texts_ind,\n",
    "    \"Inductive with code consistency\": coded_texts_ind_con,\n",
    "    \"Deductive\": coded_texts_ded\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fccbd8-2530-4be0-b396-a22f5af66b0f",
   "metadata": {},
   "source": [
    "In the relevant text extraction notebook, we evaluated the output based on an IoU measure that calculates the overlap between LLM- and human-highlighted parts of the text. A similar evaluation could be carried on the segments that are coded using methods in this notebook.\n",
    "\n",
    "In this section, we evaluate the code labels assigned to the texts. Note that the coded parts of the text may vary between annotators–as measured by IoU–which makes a highlight level evaluation of codes difficult. In order to address this issue, we compare the codes on a text (as opposed to highlight) level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c742d24-d0fe-45cb-9b83-ff001fd73176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_codes(coded_text):\n",
    "    codes = set(code for _, code in llmcode.parse_codes(coded_text))\n",
    "    return \"; \".join(codes)\n",
    "\n",
    "def get_llm_and_human_codes_by_text(df_input, coded_texts):\n",
    "    \"\"\"\n",
    "    Prepare a DataFrame containing LLM and human-generated codes for the same texts for comparison by merging all codes for each text\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for idx, row in df_input.iterrows():\n",
    "        text = row.text\n",
    "        llm_coded_text = coded_texts[idx]\n",
    "        human_coded_text = row.coded_text\n",
    "\n",
    "        # For each text, extract all LLM and human-generated codes\n",
    "        llm_codes = merge_codes(llm_coded_text) if llm_coded_text else None\n",
    "        human_codes = merge_codes(human_coded_text) if human_coded_text else None\n",
    "        \n",
    "        data.append((text, llm_codes, human_codes))\n",
    "    return pd.DataFrame(data, columns=[\"text\", \"codes\", \"human_codes\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592331a2-2422-4c60-81f7-ddb0242b9102",
   "metadata": {},
   "source": [
    "The gpt_human_code_dist() function from the LLMCode package uses text embeddings with a modified [Hausdorff distance measure](https://en.wikipedia.org/wiki/Hausdorff_distance) to evaluate the semantic similarity between the LLM and human-generated codes. Compare the average distance scores for each of the three methods (lower distance = more similar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820952a1-9f73-4122-b0e7-c7afc110d661",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_by_method = {}\n",
    "\n",
    "for method, coded_texts in coded_texts_by_method.items():\n",
    "    print(f\"Method: {method}\")\n",
    "    \n",
    "    # Combine LLM- and human-coded texts in one DataFrame\n",
    "    df_codes = get_llm_and_human_codes_by_text(df_input, coded_texts)\n",
    "\n",
    "    # Calculate semantic distances using the LLMCode package\n",
    "    df_dist = llmcode.gpt_human_code_dist(df_codes, embedding_context, embedding_model)\n",
    "    distances_by_method[method] = df_dist\n",
    "\n",
    "    # Report any texts that couldn't be coded by the LLM, as these are ignored from the mean calculation\n",
    "    nan_count = np.isnan(df_dist.dist).sum()\n",
    "    if nan_count > 0:\n",
    "        print(f\"WARNING: Excluding {nan_count} uncoded instances from analysis\")\n",
    "\n",
    "    # Calculate and display the average semantic distance across all (included) texts\n",
    "    # Ignore any np.nan values for non-coded texts\n",
    "    avg_dist = np.nanmean(df_dist.dist)\n",
    "    print(f\"Average distance: {avg_dist}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac69712-99cd-4194-b3f7-c1cf8689da2d",
   "metadata": {},
   "source": [
    "The output DataFrame rows are sorted from most similar to least similar. Inspect the DataFrames to observe what kind of texts the different LLM coding methods excel and struggle with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7a2a92-dc24-41e9-afbf-62b691ffb1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_by_method[\"Inductive\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079d1c76-baf3-4d2c-8f19-f051f8f00b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_by_method[\"Inductive with code consistency\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f217d62c-e092-4ff8-9dea-b8e58cf23b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_by_method[\"Deductive\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b142af6-7b60-4167-81b2-90a03a51b9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Table with all results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f973de26-4934-4437-82d4-397f4287eb1f",
   "metadata": {},
   "source": [
    "## Comparing diversity of LLM- and human-generated codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348a6e40-37b6-4712-af2f-39c9fb517b18",
   "metadata": {},
   "source": [
    "Both the IoU and Hausdorff distance metrics measure how well the LLM-generated codes align with human-generated codes. However, given the subjective nature of qualitative coding, one may also be interested in comparing the diversity of useful codes generated by the LLM methods. Some of these might be new codes that do not exist in the human-annotated data. One way to carry out such an analysis is to evaluate the codes visually, using the plots we generated at the end of each coding section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a76cdb9-4773-4365-907a-6dab3621f5c4",
   "metadata": {},
   "source": [
    "# Storing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668817b7-08e6-4829-b6d4-44176ac785d1",
   "metadata": {},
   "source": [
    "Run the following code to store your codes in a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67df846b-60bf-4964-9ece-d351f7e25314",
   "metadata": {},
   "outputs": [],
   "source": [
    "coded_texts_by_method = {\n",
    "    \"Inductive\": locals().get(\"coded_texts_ind\"),\n",
    "    \"Inductive with code consistency\": locals().get(\"coded_texts_ind_con\"),\n",
    "    \"Deductive\": locals().get(\"coded_texts_ded\")\n",
    "}\n",
    "\n",
    "# Create output dir\n",
    "output_dir = \"coding_output\"  # TODO: Prompt from user\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Save raw and coded texts for each method\n",
    "for name, coded_texts in coded_texts_by_method.items():\n",
    "    if coded_texts is not None:\n",
    "        data = [(t, t_coded) for t, t_coded in zip(df_input.text.tolist(), coded_texts)]\n",
    "        df_out = pd.DataFrame(data, columns=[\"text\", \"coded_text\"])\n",
    "        file_path = f\"{output_dir}/coded_texts_{name.lower().replace(\" \", \"_\")}.csv\"\n",
    "        df_out.to_csv(file_path, index=False)\n",
    "\n",
    "# Save code descriptions for ind con\n",
    "if locals().get(\"code_descriptions_ind_con\") {\n",
    "    data = code_descriptions_ind_con.items()\n",
    "    df_out = pd.DataFrame(data, columns=[\"code\", \"description\"])\n",
    "    file_path = f\"{output_dir}/code_descriptions_inductive_with_code_consistency.csv\"\n",
    "    df_out.to_csv(file_path, index=False)\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
